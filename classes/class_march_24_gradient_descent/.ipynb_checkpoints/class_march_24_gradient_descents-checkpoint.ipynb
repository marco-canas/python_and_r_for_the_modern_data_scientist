{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3065b8d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table align=\"left\">\n",
    "  <td>\n",
    "    <a href=\"https://colab.research.google.com/github/marco-canas/machine_learning/blob/main/classes/class_march_17/clase_march_17_clasificacion_multietiqueta.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "  </td>\n",
    "</table> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42390acc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradiente descendente estoc√°stico"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d38a21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31211030",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "iris = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b2e3265",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X,y = load_iris(return_X_y = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa98d2fe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb690c2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_setosa = (y==0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7678fef1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_sepal = X[:,(0,1)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cb1e43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_sepal.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33af5e22",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e19799",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X_sepal[:,0][y_setosa], X_sepal[:,1][y_setosa] )\n",
    "ax.scatter(X_sepal[:,0][y_setosa==0], X_sepal[:,1][y_setosa==0] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4095edf3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cf98d1e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(max_iter = 5, tol = 1e-3, random_state = 42) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b6fdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.fit(X_sepal, y_setosa) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c95c2222",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "b = sgd_clf.intercept_/ sgd_clf.coef_[0,1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d2da21",
   "metadata": {},
   "outputs": [],
   "source": [
    "b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38422411",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.coef_ \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4d58f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf.coef_.shape \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6039f6c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "m = -sgd_clf.coef_[0,0]/sgd_clf.coef_[0,1]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf7c63c9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "m "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0858c0e3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ w_{0} + w_{1}x_{1} + w_{2}x_{2} = 0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "814144d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ x_{2} = -\\frac{w_{1}}{w_{2}}x_{1} - \\frac{w_{0}}{w_{2}} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9851de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_clf = SGDClassifier(max_iter = 100, tol = 1e-5, random_state = 42) \n",
    "sgd_clf.fit(X_sepal, y_setosa) \n",
    "import numpy as np \n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X_sepal[:,0][y_setosa], X_sepal[:,1][y_setosa] )\n",
    "ax.scatter(X_sepal[:,0][y_setosa==0], X_sepal[:,1][y_setosa==0] )\n",
    "c,d = np.min(X[:,0]), np.max(X[:,0]) \n",
    "\n",
    "m = -sgd_clf.coef_[0,0]/sgd_clf.coef_[0,1]  \n",
    "\n",
    "b = -sgd_clf.intercept_/ sgd_clf.coef_[0,1] \n",
    "\n",
    "ax.plot([c,d], [m*c + b, m*d+b]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04732bff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Gradiente descendente para clasificaci√≥n como funci√≥n de un solo atributo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d212895",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris  \n",
    "X,y = load_iris(return_X_y = True) \n",
    "y_setosa = (y==0) \n",
    "X_sepal = X[:,0] \n",
    "import matplotlib.pyplot as plt \n",
    "fig,ax = plt.subplots()\n",
    "\n",
    "ax.scatter(X_sepal,y_setosa.ravel() )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c365a41a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def cost(w0):\n",
    "    y_pred = 1*(X_sepal<=w0)\n",
    "    return (1/len(X_sepal)*np.sum((y_setosa - y_pred)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e925ce07",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = np.min(X_sepal), np.max(X_sepal) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c70f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "W0 = np.linspace(a,b,400) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab851f10",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(W0, [cost(w) for w in W0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddaa439",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6402e397",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The Normal Equation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12d40212",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Para encontrar el valor de $\\theta$ que minimiza la funci√≥n de costo, hay una soluci√≥n de forma cerrada, en otras palabras, una ecuaci√≥n matem√°tica que da el resultado directamente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee3d5ccf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto se llama **la ecuaci√≥n normal**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5531d46",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 4-4. Normal Equation  \n",
    "\n",
    "$$ \\hat{\\theta}  = (X^{T}X)^{-1} X^{T}y $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2963be76",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "En esta ecuaci√≥n:\n",
    "\n",
    "* $\\hat{\\theta}$ is the value of $\\theta$ that minimizes the cost function.  \n",
    "\n",
    "* $y$ is the vector of target values containing $y^{(1)}$ to $y^{(m)}$ ."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f736fae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Generemos algunos datos de aspecto lineal para probar esta ecuaci√≥n (Figura 4-1):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e975abe7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42) \n",
    "\n",
    "X1 = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae78bc16",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/machine_learning/blob/main/classes/class_march_24_gradient_descent/figura_4_1.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3bb44885",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "plt.scatter(X1,y)\n",
    "plt.xlabel(r'$X_{1}$')\n",
    "plt.ylabel(r'$y$')\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a38e2ffa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Ahora calculemos $\\hat{\\theta}$ usando la ecuaci√≥n normal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b910d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Usaremos la funci√≥n `inv()` del m√≥dulo de √°lgebra lineal de NumPy (`np.linalg`) para calcular la inversa de una matriz, y el m√©todo `dot()` para la multiplicaci√≥n de matrices:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d1e216",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100, 1)), X] # add x0 = 1 to each instance\n",
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3fed670",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La funci√≥n que usamos para generar los datos es $y = 4 + 3x + \\text{ruido gaussiano}.$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d842814d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let‚Äôs see what the equation found:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6d352a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "706b8fa7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We would have hoped for $Œ∏ = 4$ and $Œ∏ = 3$ instead of $Œ∏ = 4.215$ and $Œ∏ = 2.770$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9922319",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Close enough, but the noise made it impossible to recover the exact parameters of the original function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d5ded43",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we can make predictions using $\\hat{\\theta}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3167b2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_new = np.array([[0], [2]])\n",
    "X_new_b = np.c_[np.ones((2, 1)), X_new] # add x0 = 1 to each instance\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a82e2cdf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let‚Äôs plot this model‚Äôs predictions (Figure 4-2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d47a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X_new, y_predict, \"r-\")\n",
    "plt.plot(X, y, \"b.\")\n",
    "plt.axis([0, 2, 0, 15])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9c3fa11",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Performing Linear Regression using Scikit-Learn is simple:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1797772",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "lin_reg = LinearRegression()\n",
    "lin_reg.fit(X, y)\n",
    "lin_reg.intercept_, lin_reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96330d13",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    ">>> lin_reg.predict(X_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3515af9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The LinearRegression class is based on the scipy.linalg.lstsq() function\n",
    "(the name stands for ‚Äúleast squares‚Äù), which you could call directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db051bfe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    ">>> theta_best_svd, residuals, rank, s = np.linalg.lstsq(X_b, y, rcond=1e-6)\n",
    ">>> theta_best_svd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa79b7",
   "metadata": {},
   "source": [
    "This function computes ÀÜ\n",
    "ùõâ\n",
    "= X+y, where X+ is the pseudoinverse of X\n",
    "(specifically, the Moore-Penrose inverse). You can use np.linalg.pinv() to\n",
    "compute the pseudoinverse directly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd0972c0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    ">>> np.linalg.pinv(X_b).dot(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e9569",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The pseudoinverse itself is computed using a standard matrix factorization technique called Singular Value Decomposition (SVD) that can decompose the training set matrix X into the matrix multiplication of three matrices $U Œ£ V$ (see `numpy.linalg.svd()`). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f47050",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The pseudoinverse is computed as X+ = VŒ£+U‚ä∫. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca4d0cff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To compute the matrix Œ£+, the algorithm takes Œ£ and sets to zero all values smaller than a tiny threshold value, then it replaces all the nonzero values with their inverse, and finally it transposes the resulting matrix. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb1089f3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This approach is more efficient than computing the Normal Equation, plus it handles edge cases nicely: indeed, the Normal Equation may not work if the matrix X X is not invertible (i.e., singular), such as if $m < n$ or if some features are redundant, but the pseudoinverse is always defined."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb772a4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Computational Complexity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba5e9ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The Normal Equation computes the inverse of X X, which is an (n + 1) √ó (n + 1) matrix (where n is the number of features). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0920864",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The computational complexity of inverting such a matrix is typically about O(n ) to O(n ), depending on the implementation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d120cda",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In other words, if you double the number of features, you multiply the computation time by roughly 2 = 5.3 to 2 = 8."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab1db80",
   "metadata": {},
   "source": [
    "The SVD approach used by Scikit-Learn‚Äôs LinearRegression class is about O(n ). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b464064",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you double the number of features, you multiply the computation time by roughly 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed2c78bc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce0b003",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Both the Normal Equation and the SVD approach get very slow when the number of features grows large (e.g., 100,000). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c613dd60",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On the positive side, both are linear with regard to the number of instances in the training set (they are O(m)), so they handle large training sets efficiently, provided they can fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecfde65c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Also, once you have trained your Linear Regression model (using the Normal Equation or any other algorithm), predictions are very fast: the computational complexity is linear with regard to both the number of instances you want to make predictions on and the number of features. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85712478",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In other words, making predictions on twice as many instances (or twice as many features) will take roughly twice as much time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f9545fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we will look at a very different way to train a Linear Regression model,\n",
    "which is better suited for cases where there are a large number of features or too\n",
    "many training instances to fit in memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da57a401",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Descenso de gradiente"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce12f0f1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Page: 173"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d130acc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Gradient Descent** es un algoritmo de optimizaci√≥n gen√©rico capaz de encontrar soluciones √≥ptimas a una amplia gama de problemas."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6414944e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "La idea general de **Gradient Descent** es ajustar los par√°metros iterativamente para minimizar una **funci√≥n de costo**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e38dc6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Suponga que est√° perdido en las monta√±as en una densa niebla, y solo puede sentir la pendiente del suelo debajo de sus pies."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b5d908",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una buena estrategia para llegar r√°pidamente al fondo del valle es descender en direcci√≥n a la pendiente m√°s pronunciada."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc17d6aa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto es exactamente lo que hace Gradient Descent:  \n",
    "\n",
    "* mide el gradiente local de la funci√≥n de error con respecto al vector de par√°metros $\\theta$, y va en la direcci√≥n del gradiente descendente."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d705dbf",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Una vez que el gradiente es cero, ¬°ha alcanzado un m√≠nimo!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6dca0a5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Concretamente, empiezas llenando $\\theta$ con valores aleatorios (esto se llama inicializaci√≥n aleatoria)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f4f8bf4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Luego lo mejora gradualmente, dando un peque√±o paso a la vez, cada paso intentando disminuir la funci√≥n de costo (por ejemplo, el MSE), hasta que el algoritmo converge a un m√≠nimo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2114c6b1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/machine_learning/blob/main/classes/class_march_24_gradient_descent/figura_4_3.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4688c03d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Un par√°metro importante en Gradient Descent es el tama√±o de los pasos, determinado por el hiperpar√°metro de tasa de aprendizaje (*learning rate*)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a190b96c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Si la tasa de aprendizaje es demasiado peque√±a, el algoritmo tendr√° que pasar por muchas iteraciones para converger, lo que llevar√° mucho tiempo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9329c1c5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/machine_learning/blob/main/classes/class_march_24_gradient_descent/figura_4_4_learnig_rate_small.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fca6ba6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Por otro lado, si la tasa de aprendizaje es demasiado alta, es posible que saltes al otro lado del valle y termines en el otro lado, posiblemente incluso m√°s alto de lo que estabas antes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34dbf040",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Esto podr√≠a hacer que el algoritmo diverja, con valores cada vez mayores, y no pueda encontrar una buena soluci√≥n."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d689eb0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/machine_learning/blob/main/classes/class_march_24_gradient_descent/figura_4_5_learning_rate_large.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6cfd39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Finalmente, no todas las **funciones de costos** se ven como tazones regulares y agradables."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70e73bac",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "There may be holes, ridges, plateaus, and all sorts of irregular terrains, making convergence to the minimum difficult. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8cb553",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Figure 4-6 shows the two main challenges with Gradient Descent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33daf1bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the random initialization starts the algorithm on the left, then it will converge to a local minimum, which is not as good as the global minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78c84e90",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If it starts on the right, then it will take a very long time to cross the plateau. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f36b79b7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "And if you stop too early, you will never reach the global minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d12d4cf4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5915d40",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fortunately, the MSE cost function for a Linear Regression model happens to be a convex function, which means that if you pick any two points on the curve, the line segment joining them never crosses the curve. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c3d8dc1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This implies that there are no local minima, just one global minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa739a4c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is also a continuous function with a slope that never changes abruptly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cd40b9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "These two facts have a great consequence: Gradient Descent is guaranteed to approach arbitrarily close the global minimum (if you wait long enough and if the learning rate is not too high)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d696973",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In fact, the cost function has the shape of a bowl, but it can be an elongated bowl\n",
    "if the features have very different scales. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a14640a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Figure 4-7 shows Gradient Descent on a training set where features 1 and 2 have the same scale (on the left), and on a training set where feature 1 has much smaller values than feature 2 (on the right)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c05c9a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca183046",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "As you can see, on the left the Gradient Descent algorithm goes straight toward the minimum, thereby reaching it quickly, whereas on the right it first goes in a direction almost orthogonal to the direction of the global minimum, and it ends with a long march down an almost flat valley. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edceb54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It will eventually reach the minimum, but it will take a long time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af2badc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## WARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a368c15c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When using Gradient Descent, you should ensure that all features have a similar scale (e.g.,\n",
    "using Scikit-Learn‚Äôs StandardScaler class), or else it will take much longer to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7698383",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This diagram also illustrates the fact that training a model means searching for a combination of model parameters that minimizes a cost function (over the training set). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07df70dd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It is a search in the model‚Äôs parameter space: the more parameters a model has, the more dimensions this space has, and the harder the search is: searching for a needle in a 300-dimensional haystack is much trickier than in 3\n",
    "dimensions. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a23b078",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Fortunately, since the cost function is convex in the case of Linear\n",
    "Regression, the needle is simply at the bottom of the bowl."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce40f253",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b95ad7b4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To implement Gradient Descent, you need to compute the gradient of the cost function with regard to each model parameter Œ∏ . "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87a73f2f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In other words, you need to calculate how much the cost function will change if you change Œ∏ just a little bit."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5623aea7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is called a partial derivative. It is like asking ‚ÄúWhat is the slope of the mountain under my feet if I face east?‚Äù and then asking the same question facing north (and so on for all other dimensions, if you can imagine a universe with more than three dimensions). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f76d0823",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Equation 4-5 computes the partial derivative of the cost function with regard to parameter Œ∏ , noted \n",
    "\n",
    "$$ \\frac{\\partial}{\\partial \\theta_{j}} MSE(\\theta). $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9009f9b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\frac{\\partial}{\\partial \\theta_{j}} MSE(\\theta) = \\frac{2}{m} \\sum   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a95c6bf0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Instead of computing these partial derivatives individually, you can use Equation\n",
    "4-6 to compute them all in one go. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7cb5ab9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The gradient vector, noted $\\nabla_{\\theta} MSE(\\theta)$, contains all the partial derivatives of the cost function (one for each model parameter)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac84b3b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "$$ \\nabla_{\\theta}   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61ab9e5",
   "metadata": {},
   "source": [
    "Once you have the gradient vector, which points uphill, just go in the opposite\n",
    "direction to go downhill. This means subtracting ‚àá MSE(Œ∏) from Œ∏. This is\n",
    "where the learning rate Œ∑ comes into play: multiply the gradient vector by Œ∑ to\n",
    "determine the size of the downhill step (Equation 4-7).\n",
    "\n",
    "$$ \\theta^{\\text{next step}} =  $$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b192d6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let‚Äôs look at a quick implementation of this algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3622b773",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_b' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12396/2650310110.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# random initialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0miteration\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_iterations\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mgradients\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mX_b\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_b\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtheta\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtheta\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0meta\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgradients\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_b' is not defined"
     ]
    }
   ],
   "source": [
    "eta = 0.1 # learning rate\n",
    "n_iterations = 1000\n",
    "m = 100\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "for iteration in range(n_iterations):\n",
    "    gradients = 2/m * X_b.T.dot(X_b.dot(theta) - y)\n",
    "    theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8900b198",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "That wasn‚Äôt too hard! Let‚Äôs look at the resulting theta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe148d6a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "theta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7868f235",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Hey, that‚Äôs exactly what the Normal Equation found! Gradient Descent worked perfectly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6e8c21",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But what if you had used a different learning rate eta? Figure 4-8 shows the first 10 steps of Gradient Descent using three different learning rates (the dashed line represents the starting point)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52bdaab6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = ''>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ee80b89",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On the left, the learning rate is too low: the algorithm will eventually reach the solution, but it will take a long time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96022df3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In the middle, the learning rate looks pretty good: in just a few iterations, it has already converged to the solution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61878377",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On the right, the learning rate is too high: the algorithm diverges, jumping all over the\n",
    "place and actually getting further and further away from the solution at every step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dccc57a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "To find a good learning rate, you can use grid search (see Chapter 2). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b64523",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "However,\n",
    "you may want to limit the number of iterations so that grid search can eliminate\n",
    "models that take too long to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c94ecb5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "You may wonder how to set the number of iterations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "399e7d52",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If it is too low, you will still be far away from the optimal solution when the algorithm stops; but if it is too high, you will waste time while the model parameters do not change\n",
    "anymore. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4009116",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A simple solution is to set a very large number of iterations but to interrupt the algorithm when the gradient vector becomes tiny‚Äîthat is, when its norm becomes smaller than a tiny number $œµ$ (called the tolerance)‚Äîbecause this happens when Gradient Descent has (almost) reached the minimum."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6866e0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### CONVERGENCE RATE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a978bdff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When the cost function is convex and its slope does not change abruptly (as is the case for the MSE cost function), Batch Gradient Descent with a fixed learning rate will eventually converge to the optimal solution, but you may have to wait a while: it can take $O(1/œµ)$ iterations to reach the optimum within a range of $œµ$, depending on the shape of the cost function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3131ad49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If you divide the tolerance by 10 to have a more precise solution, then the algorithm may have to run about 10 times longer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47141f54",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b33b49",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The main problem with Batch Gradient Descent is the fact that it uses the whole training set to compute the gradients at every step, which makes it very slow\n",
    "when the training set is large. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6daf4a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "At the opposite extreme, Stochastic Gradient Descent picks a random instance in the training set at every step and computes the gradients based only on that single instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5db396f0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Obviously, working on a single instance at a time makes the algorithm much faster because it has very little data to manipulate at every iteration. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cf05a7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "It also makes it possible to train on huge training sets, since only one instance needs to be in memory at each iteration (Stochastic GD can be implemented as an out-of-core algorithm; see Chapter 1)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8868fe73",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "On the other hand, due to its stochastic (i.e., random) nature, this algorithm is much less regular than Batch Gradient Descent: instead of gently decreasing until it reaches the minimum, the cost function will bounce up and down, decreasing only on average. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b04850",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Over time it will end up very close to the minimum, but once it gets there it will continue to bounce around, never settling down (see Figure 4-9). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21792d4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So once the algorithm stops, the final parameter values are good, but not optimal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b82ec1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = 'https://github.com/marco-canas/machine_learning/blob/main/classes/class_march_24_gradient_descent/figura_4_9.jpg?raw=true'>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242786de",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "When the cost function is very irregular (as in Figure 4-6), this can actually help the algorithm jump out of local minima, so Stochastic Gradient Descent has a better chance of finding the global minimum than Batch Gradient Descent does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3dab3ba",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Therefore, randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991b8dd4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "One solution to this dilemma is to gradually reduce the learning rate. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894833c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The steps start out large (which helps make quick progress and escape local minima), then get smaller and smaller, allowing the algorithm to settle at the global minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "977d1e68",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This process is akin to simulated annealing, an algorithm inspired from the process in\n",
    "metallurgy of annealing, where molten metal is slowly cooled down. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be93da7c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The function that determines the learning rate at each iteration is called the learning\n",
    "schedule. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fe8a7f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the learning rate is reduced too quickly, you may get stuck in a local minimum, or even end up frozen halfway to the minimum. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64227790",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If the learning rate is reduced too slowly, you may jump around the minimum for a long time and end up with a suboptimal solution if you halt training too early."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ea2970",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This code implements Stochastic Gradient Descent using a simple learning\n",
    "schedule:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f9b7f052",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12396/2381621019.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mtheta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# random initialization\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mrandom_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0mxi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_b\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mrandom_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mrandom_index\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'm' is not defined"
     ]
    }
   ],
   "source": [
    "n_epochs = 50\n",
    "t0, t1 = 5, 50 # learning schedule hyperparameters\n",
    "def learning_schedule(t):\n",
    "    return t0 / (t + t1)\n",
    "theta = np.random.randn(2,1) # random initialization\n",
    "for epoch in range(n_epochs):\n",
    "    for i in range(m):\n",
    "        random_index = np.random.randint(m)\n",
    "        xi = X_b[random_index:random_index+1]\n",
    "        yi = y[random_index:random_index+1]\n",
    "        gradients = 2 * xi.T.dot(xi.dot(theta) - yi)\n",
    "        eta = learning_schedule(epoch * m + i)\n",
    "        theta = theta - eta * gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d7196c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6ff744",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e8f0af06",
   "metadata": {},
   "source": [
    "## Referencias  \n",
    "\n",
    "* https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9adb07f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
